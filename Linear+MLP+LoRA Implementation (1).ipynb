{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d556ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4775b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "           \"hours-per-week\", \"native-country\", \"income\"]\n",
    "\n",
    "df = pd.read_csv(url, header=None, names=columns, skipinitialspace=True)  # Trim spaces\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(columns=[\"fnlwgt\"], inplace=True)\n",
    "\n",
    "# Convert categorical variables to one-hot encoding\n",
    "categorical_features = [\"workclass\", \"education\", \"marital-status\", \"occupation\",\n",
    "                        \"relationship\", \"race\", \"native-country\", \"sex\"]\n",
    "df = pd.get_dummies(df, columns=categorical_features, dtype=float)  # Ensure float dtype\n",
    "\n",
    "# Convert income to binary\n",
    "df[\"income\"] = df[\"income\"].map(lambda x: 0 if x == \"<=50K\" else 1)\n",
    "\n",
    "X= df.drop(columns=[\"income\"]).values\n",
    "y= df[\"income\"].values\n",
    "\n",
    "X= torch.tensor(X, dtype=torch.float32)\n",
    "y= torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "X_finetune, X_test, y_finetune, Y_test = train_test_split(X_temp, y_temp, test_size=0.3333, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train),batch_size=32,shuffle=True)\n",
    "finetune_loader = DataLoader(TensorDataset(X_temp, y_temp), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, Y_test), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ee7bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecc0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return torch.sigmoid(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ebbed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs=10, lr=0.001):\n",
    "   optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "   criterion = nn.BCELoss()\n",
    "   model.train()\n",
    "\n",
    "   for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred.squeeze(), y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690d8c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Model\n",
      "Epoch 1, Loss: 3.5928\n",
      "Epoch 2, Loss: 3.4600\n",
      "Epoch 3, Loss: 4.5436\n",
      "Epoch 4, Loss: 4.6617\n",
      "Epoch 5, Loss: 4.1413\n",
      "Epoch 6, Loss: 3.0217\n",
      "Epoch 7, Loss: 3.0249\n",
      "Epoch 8, Loss: 4.0635\n",
      "Epoch 9, Loss: 4.0833\n",
      "Epoch 10, Loss: 3.9214\n",
      "Training MLP Model\n",
      "Epoch 1, Loss: 1.1066\n",
      "Epoch 2, Loss: 1.3547\n",
      "Epoch 3, Loss: 1.0321\n",
      "Epoch 4, Loss: 0.4341\n",
      "Epoch 5, Loss: 1.8345\n",
      "Epoch 6, Loss: 1.4303\n",
      "Epoch 7, Loss: 0.3670\n",
      "Epoch 8, Loss: 0.4139\n",
      "Epoch 9, Loss: 0.3761\n",
      "Epoch 10, Loss: 0.3739\n"
     ]
    }
   ],
   "source": [
    "input_dim = X.shape[1]\n",
    "output_dim = len(torch.unique(y))\n",
    "\n",
    "linear_model = LinearModel(input_dim)\n",
    "mlp_model = TwoLayerNN(input_dim, hidden_dim=32)\n",
    "\n",
    "print(\"Training Linear Model\")\n",
    "train_model(linear_model, train_loader)\n",
    "\n",
    "print(\"Training MLP Model\")\n",
    "train_model(mlp_model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37d85a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "\n",
    "# Modify the model to use LoRA in Linear Layers\n",
    "class LoRAMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32):\n",
    "        super(LoRAMLP, self).__init__()\n",
    "        self.fc1 = lora.Linear(input_dim, hidden_dim, r=8)  # Low-rank adaptation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = lora.Linear(hidden_dim, 1, r=8)  # LoRA applied\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a90022ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Linear Model with LoRA...\n",
      "Epoch 1, Loss: 2.6496\n",
      "Epoch 2, Loss: 2.9593\n",
      "Epoch 3, Loss: 3.0333\n",
      "Epoch 4, Loss: 3.0464\n",
      "Epoch 5, Loss: 2.9925\n",
      "Epoch 6, Loss: 3.6039\n",
      "Epoch 7, Loss: 2.9815\n",
      "Epoch 8, Loss: 3.3733\n",
      "Epoch 9, Loss: 2.7831\n",
      "Epoch 10, Loss: 5.1741\n",
      "Fine-tuning MLP Model with LoRA...\n",
      "Epoch 1, Loss: 1.4118\n",
      "Epoch 2, Loss: 0.3808\n",
      "Epoch 3, Loss: 0.3730\n",
      "Epoch 4, Loss: 0.3743\n",
      "Epoch 5, Loss: 0.3622\n",
      "Epoch 6, Loss: 0.3822\n",
      "Epoch 7, Loss: 0.3667\n",
      "Epoch 8, Loss: 0.3593\n",
      "Epoch 9, Loss: 0.3623\n",
      "Epoch 10, Loss: 0.3704\n",
      "Epoch 11, Loss: 0.4453\n",
      "Epoch 12, Loss: 0.3527\n",
      "Epoch 13, Loss: 0.3542\n",
      "Epoch 14, Loss: 0.3564\n",
      "Epoch 15, Loss: 0.3835\n",
      "Epoch 16, Loss: 0.3569\n",
      "Epoch 17, Loss: 0.3491\n",
      "Epoch 18, Loss: 0.3524\n",
      "Epoch 19, Loss: 0.3534\n",
      "Epoch 20, Loss: 1.5519\n"
     ]
    }
   ],
   "source": [
    "print(\"Fine-tuning Linear Model with LoRA...\")\n",
    "train_model(linear_model, finetune_loader, lr=0.001)\n",
    "\n",
    "print(\"Fine-tuning MLP Model with LoRA...\")\n",
    "mlp_model_lora = LoRAMLP(input_dim, hidden_dim=32)\n",
    "train_model(mlp_model_lora, finetune_loader, epochs=20, lr=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bcbab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(predictions.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e1f9eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Linear Model...\n",
      "Linear Model Accuracy: 0.7566\n",
      "Evaluating MLP Model...\n",
      "MLP Model Accuracy: 0.7566\n",
      "Evaluating MLP Model with LoRA...\n",
      "MLP Model with LoRA Accuracy: 0.7566\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Linear Model...\")\n",
    "acc_linear = evaluate_model(linear_model, test_loader)\n",
    "print(f\"Linear Model Accuracy: {acc_linear:.4f}\")\n",
    "\n",
    "print(\"Evaluating MLP Model...\")\n",
    "acc_mlp = evaluate_model(mlp_model, test_loader)\n",
    "print(f\"MLP Model Accuracy: {acc_mlp:.4f}\")\n",
    "\n",
    "print(\"Evaluating MLP Model with LoRA...\")\n",
    "acc_mlp_lora = evaluate_model(mlp_model_lora, test_loader)\n",
    "print(f\"MLP Model with LoRA Accuracy: {acc_mlp_lora:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c3aaffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Accuracy Comparison:\n",
      "Linear Model: 75.6603%\n",
      "MLP Model: 75.6603%\n",
      "MLP + LoRA: 75.6603%\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"Linear Model\": acc_linear,\n",
    "    \"MLP Model\": acc_mlp,\n",
    "    \"MLP + LoRA\": acc_mlp_lora\n",
    "}\n",
    "print(\"\\nFinal Accuracy Comparison:\")\n",
    "for model, acc in results.items():\n",
    "    print(f\"{model}: {acc:.4%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
